# RFC: Otimiza√ß√µes da Codebase BR-Statistics Hub

**Status:** Em Implementa√ß√£o (Parcialmente Conclu√≠do)
**Data:** 2026-01-31  
**Autor:** Sistema  

---

## üìã Sum√°rio Executivo

Este documento prop√µe otimiza√ß√µes cr√≠ticas e incrementais para a codebase do BR-Statistics Hub, focando em **performance**, **escalabilidade**, **seguran√ßa** e **manutenibilidade**.

---

## üéØ Otimiza√ß√µes Cr√≠ticas (Alta Prioridade)

### 1. **Substituir JSON File Storage por Redis**

**Problema:** `data/scrape_jobs.json` n√£o √© thread-safe e n√£o escala

**Proposta:**
```python
# Antes: app/routes/scrape.py
jobs = json.load(open(JOBS_FILE))  # Race condition!

# Depois: Redis
import redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)
r.hset('jobs', job_id, json.dumps(job_data))
```

**Benef√≠cios:**
- ‚úÖ Thread-safe (opera√ß√µes at√¥micas)
- ‚úÖ Pub/Sub para notifica√ß√µes em tempo real
- ‚úÖ TTL autom√°tico para limpeza de jobs antigos
- ‚úÖ Suporta m√∫ltiplas inst√¢ncias da API

**Esfor√ßo:** 4h | **ROI:** Alto

---

### 2. **Async Database Queries com SQLAlchemy + asyncpg** (‚úÖ IMPLEMENTADO v3.4.0)

**Problema:** Queries s√≠ncronas bloqueiam o event loop do Flask

**Proposta:**
```python
# Antes: app/routes/matches.py
matches = Partida.query.filter_by(rodada=round_num).all()  # Blocking!

# Depois: Async
from sqlalchemy.ext.asyncio import AsyncSession
async def get_matches(round_num: int):
    async with AsyncSession(engine) as session:
        result = await session.execute(
            select(Partida).where(Partida.rodada == round_num)
        )
        return result.scalars().all()
```

**Benef√≠cios:**
- ‚úÖ Melhor throughput da API (10-50x mais requests/segundo)
- ‚úÖ Reduz lat√™ncia de I/O
- ‚úÖ Permite conex√µes persistentes

**Esfor√ßo:** 8h | **ROI:** Muito Alto

---

### 3. **Implementar Cache Layer (Redis)** (‚úÖ IMPLEMENTADO v3.3.0)

**Problema:** Queries repetitivas sem cache

**Proposta:**
```python
from flask_caching import Cache

cache = Cache(app, config={
    'CACHE_TYPE': 'redis',
    'CACHE_REDIS_URL': 'redis://localhost:6379/1',
    'CACHE_DEFAULT_TIMEOUT': 300
})

@app.route('/api/teams')
@cache.cached(timeout=3600, query_string=True)
def list_teams():
    return Team.query.all()
```

**Benef√≠cios:**
- ‚úÖ Reduz carga no PostgreSQL (90%)
- ‚úÖ Resposta API: 500ms ‚Üí 5ms
- ‚úÖ Cache invalidation autom√°tico

**Esfor√ßo:** 3h | **ROI:** Muito Alto

---

### 4. **N+1 Queries - Eager Loading** (‚úÖ IMPLEMENTADO v3.4.0)

**Problema:** Queries N+1 em `/api/matches/{id}`

**An√°lise:**
```python
# Antes (N+1):
match = Partida.query.get(1)  # 1 query
match.time_casa.nome          # +1 query
match.time_fora.nome          # +1 query
match.estadio.nome            # +1 query
# Total: 4 queries para 1 partida!

# Depois (Eager Loading):
match = Partida.query.options(
    joinedload(Partida.time_casa),
    joinedload(Partida.time_fora),
    joinedload(Partida.estadio)
).get(1)
# Total: 1 query!
```

**Benef√≠cios:**
- ‚úÖ Reduz lat√™ncia: 200ms ‚Üí 20ms
- ‚úÖ Menos overhead de rede DB

**Esfor√ßo:** 2h | **ROI:** Alto

---

### 5. **Idempotent Scraping Jobs** (‚úÖ IMPLEMENTADO v3.1.0)

**Problema:** `is_duplicate` verifica PIDs que podem ser reciclados

**Proposta:**
```python
# Antes:
if is_process_running(job['pid']):  # PID pode ser reciclado!
    return 409

# Depois: UUID + Lock sem√¢ntico
import uuid
job_key = f"{league}:{year}:{round}"
if r.set(f"lock:{job_key}", uuid.uuid4(), nx=True, ex=3600):
    # Acquired lock, pode processar
else:
    return 409, "Job already processing"
```

**Benef√≠cios:**
- ‚úÖ Evita duplicatas reais
- ‚úÖ Lock distribu√≠do (m√∫ltiplas inst√¢ncias)
- ‚úÖ TTL autom√°tico (self-healing)

**Esfor√ßo:** 2h | **ROI:** M√©dio

---

## üîß Otimiza√ß√µes de Performance (M√©dia Prioridade)

### 6. **Connection Pool Tuning** (‚úÖ IMPLEMENTADO v3.1.0)

**Proposta:**
```python
# config.py
SQLALCHEMY_ENGINE_OPTIONS = {
    'pool_size': 20,          # Antes: 5
    'max_overflow': 40,       # Antes: 10
    'pool_pre_ping': True,    # Health check
    'pool_recycle': 3600      # Evita conex√µes stale
}
```

**Benef√≠cios:**
- ‚úÖ Reduz lat√™ncia de conex√£o
- ‚úÖ Suporta mais requisi√ß√µes simult√¢neas

**Esfor√ßo:** 0.5h | **ROI:** M√©dio

---

### 7. **Scraper: Rate Limiting Inteligente**

**Problema:** Delays fixos (2s) s√£o ineficientes

**Proposta:**
```python
# Antes:
time.sleep(2)  # Sempre 2s, mesmo se site est√° r√°pido

# Depois: Adaptive throttling
class AdaptiveThrottle:
    def __init__(self, min_delay=0.5, max_delay=5):
        self.delays = deque(maxlen=10)
        self.min_delay = min_delay
        self.max_delay = max_delay
    
    def wait(self, response_time):
        # Se site est√° lento, diminui velocidade
        delay = max(self.min_delay, min(response_time * 1.5, self.max_delay))
        time.sleep(delay)
        self.delays.append(delay)
```

**Benef√≠cios:**
- ‚úÖ Scraping 30-50% mais r√°pido quando poss√≠vel
- ‚úÖ Respeita limites do servidor (n√£o sobrecarrega)

**Esfor√ßo:** 3h | **ROI:** M√©dio

---

### 8. **Playwright: Reutilizar Browser Context**

**Problema:** `browser.launch()` a cada partida (lento!)

**Proposta:**
```python
# Antes:
with sync_playwright() as p:
    browser = p.chromium.launch()  # 2-3s CADA vez!
    page = browser.new_page()
    
# Depois: Persistent context
@contextmanager
def get_browser_pool():
    with sync_playwright() as p:
        browser = p.chromium.launch()
        try:
            yield browser
        finally:
            browser.close()

# Reutiliza browser para todas as partidas da rodada
with get_browser_pool() as browser:
    for url in match_urls:
        page = browser.new_page()
        scrape_match(page, url)
        page.close()
```

**Benef√≠cios:**
- ‚úÖ Reduz overhead: ~30s por rodada
- ‚úÖ Menos uso de mem√≥ria

**Esfor√ßo:** 2h | **ROI:** Alto

---

## üõ°Ô∏è Otimiza√ß√µes de Seguran√ßa

### 9. **Rate Limiting por IP/User**

**Problema:** Flask-Limiter configurado, mas n√£o aplicado em scraping endpoints

**Proposta:**
```python
from flask_limiter import Limiter

limiter = Limiter(
    app,
    key_func=get_remote_address,
    storage_uri="redis://localhost:6379"
)

@scrape_bp.route('', methods=['POST'])
@limiter.limit("5 per minute")  # Previne spam
def start_scrape():
    ...
```

**Esfor√ßo:** 1h | **ROI:** M√©dio

---

### 10. **Secrets Management**

**Problema:** `.env` commitado em alguns casos

**Proposta:**
```bash
# Docker Secrets / Kubernetes Secrets
docker secret create db_url postgres://...

# Ou: HashiCorp Vault
vault kv put secret/br-stats DATABASE_URL=...
```

**Esfor√ßo:** 4h | **ROI:** Alto (Compliance)

---

## üìä Otimiza√ß√µes de Arquitetura

### 11. **Event-Driven Architecture (Webhook Notifications)**

**Proposta:**
```python
# Quando job completa, dispara webhook
requests.post(
    webhook_url,
    json={
        'event': 'scrape.completed',
        'job_id': job_id,
        'matches_scraped': 10
    }
)
```

**Benef√≠cios:**
- ‚úÖ Integra√ß√£o com CI/CD
- ‚úÖ Notifica√ß√µes em tempo real
- ‚úÖ Extensibilidade

**Esfor√ßo:** 5h | **ROI:** M√©dio

---

### 12. **Separar Worker em Processo Independente (Celery)**

**Problema:** Worker thread compartilha mem√≥ria com API

**Proposta:**
```python
# worker.py (processo separado)
from celery import Celery

app = Celery('scraper', broker='redis://localhost:6379/0')

@app.task
def scrape_job(league, year, round):
    run_batch(league, year, round)

# api.py
@scrape_bp.route('', methods=['POST'])
def start_scrape():
    task = scrape_job.delay(league, year, round)
    return jsonify({'task_id': task.id}), 202
```

**Benef√≠cios:**
- ‚úÖ Isolamento de falhas (worker crash ‚â† API crash)
- ‚úÖ Escalabilidade horizontal (m√∫ltiplos workers)
- ‚úÖ Retry autom√°tico (Celery built-in)
- ‚úÖ Monitoring (Flower dashboard)

**Esfor√ßo:** 8h | **ROI:** Muito Alto

---

### 13. **Feature Flags (LaunchDarkly / Unleash)**

**Proposta:**
```python
from unleash import UnleashClient

client = UnleashClient(url="http://unleash:4242", app_name="br-stats")

if client.is_enabled("new_scraper_v2"):
    scraper = NewScraperV2()
else:
    scraper = OldScraper()
```

**Benef√≠cios:**
- ‚úÖ Deploy confiante (rollback f√°cil)
- ‚úÖ A/B testing
- ‚úÖ Canary releases

**Esfor√ßo:** 6h | **ROI:** M√©dio

---

## üß™ Otimiza√ß√µes de Testes

### 14. **VCR.py para Testes de Scraper**

**Problema:** Testes de scraper dependem de site externo

**Proposta:**
```python
import vcr

@vcr.use_cassette('fixtures/match_123.yaml')
def test_scrape_match():
    data = scraper.scrape('https://ogol.com.br/jogo/...')
    assert data['home_team'] == 'Flamengo'
```

**Benef√≠cios:**
- ‚úÖ Testes determin√≠sticos
- ‚úÖ CI/CD n√£o depende de ogol.com.br
- ‚úÖ Mais r√°pido (sem rede)

**Esfor√ßo:** 4h | **ROI:** Alto

---

## üìà Otimiza√ß√µes de Observabilidade

### 15. **OpenTelemetry + Jaeger**

**Proposta:**
```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@tracer.start_as_current_span("scrape_match")
def scrape_match(url):
    with tracer.start_as_current_span("fetch_page"):
        page.goto(url)
    with tracer.start_as_current_span("extract_data"):
        data = extract_match_info(page)
```

**Benef√≠cios:**
- ‚úÖ Identifica gargalos (tracing distribu√≠do)
- ‚úÖ Correla√ß√£o de logs
- ‚úÖ SLA monitoring

**Esfor√ßo:** 6h | **ROI:** Alto

---

### 16. **Prometheus Metrics**

**Proposta:**
```python
from prometheus_client import Counter, Histogram

scrape_duration = Histogram('scrape_duration_seconds', 'Scraping duration')
scrape_errors = Counter('scrape_errors_total', 'Total scraping errors')

@scrape_duration.time()
def scrape_match(url):
    try:
        ...
    except Exception:
        scrape_errors.inc()
        raise
```

**Benef√≠cios:**
- ‚úÖ Alertas (Grafana)
- ‚úÖ SLO/SLI tracking
- ‚úÖ Capacity planning

**Esfor√ßo:** 4h | **ROI:** Alto

---

## üóÇÔ∏è Otimiza√ß√µes de DB Schema

### 17. **Indexa√ß√£o Otimizada**

**An√°lise:**
```sql
-- Adicionar √≠ndices compostos estrat√©gicos
CREATE INDEX idx_partidas_season_round ON partidas(season_id, rodada);
CREATE INDEX idx_partidas_teams ON partidas(time_casa_id, time_fora_id);
CREATE INDEX idx_team_seasons_lookup ON team_seasons(team_id, season_id);

-- √çndice parcial para queries comuns
CREATE INDEX idx_active_seasons ON seasons(league_id) WHERE is_current = true;
```

**Benef√≠cios:**
- ‚úÖ Query speed: 500ms ‚Üí 20ms
- ‚úÖ Suporta mais filtros simult√¢neos

**Esfor√ßo:** 2h | **ROI:** Alto

---

### 18. **Particionamento de Tabela `partidas`**

**Proposta:**
```sql
-- Particionar por season_id
CREATE TABLE partidas_2024 PARTITION OF partidas
    FOR VALUES IN (1, 2, 3);  -- season_ids de 2024

CREATE TABLE partidas_2025 PARTITION OF partidas
    FOR VALUES IN (4, 5, 6);  -- season_ids de 2025
```

**Benef√≠cios:**
- ‚úÖ Queries 3-5x mais r√°pidas (partition pruning)
- ‚úÖ Backup/restore mais r√°pido
- ‚úÖ DELETE old data mais eficiente

**Esfor√ßo:** 6h | **ROI:** M√©dio (escala futura)

---

## üì¶ Roadmap Sugerido

### **Fase 1 - Quick Wins (1-2 semanas)**
1. ‚úÖ N+1 Queries (Eager Loading)
2. ‚úÖ Connection Pool Tuning
3. ‚úÖ Cache Layer (Redis)
4. ‚úÖ Indexa√ß√£o DB

**ROI:** 50% melhoria de performance

### **Fase 2 - Funda√ß√£o (3-4 semanas)**
1. ‚úÖ Async SQLAlchemy
2. ‚úÖ Celery Worker
3. ‚úÖ Redis Job Storage (Completed)
4. ‚úÖ Rate Limiting

**ROI:** 200% melhoria + escalabilidade horizontal

### **Fase 3 - Maturidade (2 meses)**
1. ‚úÖ OpenTelemetry
2. ‚úÖ Feature Flags
3. ‚úÖ Secrets Management
4. ‚úÖ Table Partitioning

**ROI:** Produ√ß√£o enterprise-ready

---

## üí∞ An√°lise de Custo-Benef√≠cio

| Otimiza√ß√£o | Esfor√ßo | ROI | Prioridade |
|------------|---------|-----|------------|
| Cache Layer | 3h | üî• Muito Alto | P0 |
| Async DB | 8h | üî• Muito Alto | P0 |
| N+1 Queries | 2h | üî• Alto | P0 |
| Celery Worker | 8h | üî• Muito Alto | P1 |
| Redis Jobs | 4h | üî• Alto | P1 |
| Indexa√ß√£o DB | 2h | üî• Alto | P1 |
| Browser Pool | 2h | üî• Alto | P2 |
| OpenTelemetry | 6h | Alto | P2 |
| Partitioning | 6h | M√©dio | P3 |

---

## üé¨ Pr√≥ximos Passos

1. **Review desta RFC** com equipe
2. **Priorizar itens** (voting)
3. **Criar issues** no GitHub
4. **PoC de Async SQLAlchemy** (validar benef√≠cios)
5. **Implementar Fase 1** (quick wins)

---

## üìö Refer√™ncias

- [Flask Performance Best Practices](https://flask.palletsprojects.com/en/latest/deploying/)
- [SQLAlchemy Async](https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html)
- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [PostgreSQL Indexing](https://www.postgresql.org/docs/current/indexes.html)
